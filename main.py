# coding=utf-8
# Copyright 2020 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Training and evaluation"""

import os
# only cuda 0 and 1 are available
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import sys
# add inverse folder to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'inverse'))



import run_lib
import run_lib_reflow
import run_lib_pytorch
from absl import app
from absl import flags
from ml_collections.config_flags import config_flags
import logging

import tensorflow as tf


FLAGS = flags.FLAGS

config_flags.DEFINE_config_file(
  "config", None, "Training configuration.", lock_config=True)
flags.DEFINE_string("workdir", None, "Work directory.")
flags.DEFINE_enum("mode", None, ["train", "eval", "reflow", 'eval_inverse'], "Running mode")
flags.DEFINE_string("eval_folder", "eval",
                    "The folder name for storing evaluation results")
flags.mark_flags_as_required(["workdir", "config", "mode"])


def main(argv):
  if FLAGS.mode == "train":
    # Create the working directory
    tf.io.gfile.makedirs(FLAGS.workdir)
    # Set logger so that it outputs to both console and file
    # Make logging work for both disk and Google Cloud Storage
    gfile_stream = open(os.path.join(FLAGS.workdir, 'stdout.txt'), 'w')
    handler = logging.StreamHandler(gfile_stream)
    formatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')
    handler.setFormatter(formatter)
    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel('INFO')
    # Run the training pipeline
    if 'pytorch' in FLAGS.config.data.dataset.lower():
        run_lib_pytorch.train(FLAGS.config, FLAGS.workdir)
    else:
        run_lib.train(FLAGS.config, FLAGS.workdir)
  elif FLAGS.mode == "eval":
    # Run the evaluation pipeline
    if 'pytorch' in FLAGS.config.data.dataset.lower():
        run_lib_pytorch.evaluate(FLAGS.config, FLAGS.workdir, FLAGS.eval_folder)
    else:
        run_lib.evaluate(FLAGS.config, FLAGS.workdir, FLAGS.eval_folder)
  elif FLAGS.mode == 'eval_inverse':
    # Create the working directory
    tf.io.gfile.makedirs(os.path.join(FLAGS.workdir, FLAGS.eval_folder))
    # Set logger so that it outputs to both console and file
    # Make logging work for both disk and Google Cloud Storage
    gfile_stream = open(os.path.join(FLAGS.workdir, FLAGS.eval_folder, 'stdout.txt'), 'w')
    handler = logging.StreamHandler(gfile_stream)
    formatter = logging.Formatter('%(levelname)s - %(filename)s - %(asctime)s - %(message)s')
    handler.setFormatter(formatter)
    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel('INFO')
    if 'pytorch' in FLAGS.config.data.dataset.lower():
      run_lib_pytorch.evaluate_inverse(FLAGS.config, FLAGS.workdir, FLAGS.eval_folder)
    else:
      run_lib.evaluate_inverse(FLAGS.config, FLAGS.workdir, FLAGS.eval_folder)
  elif  FLAGS.mode == "reflow":
    run_lib_reflow.finetune_reflow(FLAGS.config, FLAGS.workdir)
  else:
    raise ValueError(f"Mode {FLAGS.mode} not recognized.")


if __name__ == "__main__":
  gpus = tf.config.experimental.list_physical_devices('GPU')
  for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
  app.run(main)
